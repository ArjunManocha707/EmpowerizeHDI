{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BFHyfsjg46T3",
        "outputId": "6c1553f9-27af-494b-fdd7-c373a7f55846"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "ERROR: Could not find a version that satisfies the requirement torch==2.0.1 (from versions: none)\n",
            "ERROR: No matching distribution found for torch==2.0.1\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Note: you may need to restart the kernel to use updated packages.Collecting transformers==4.33.2\n",
            "  Using cached transformers-4.33.2-py3-none-any.whl.metadata (119 kB)\n",
            "Collecting filelock (from transformers==4.33.2)\n",
            "  Using cached filelock-3.13.1-py3-none-any.whl.metadata (2.8 kB)\n",
            "Collecting huggingface-hub<1.0,>=0.15.1 (from transformers==4.33.2)\n",
            "  Using cached huggingface_hub-0.19.4-py3-none-any.whl.metadata (14 kB)\n",
            "Requirement already satisfied: numpy>=1.17 in c:\\users\\arjun\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers==4.33.2) (1.26.2)\n",
            "Requirement already satisfied: packaging>=20.0 in c:\\users\\arjun\\appdata\\roaming\\python\\python312\\site-packages (from transformers==4.33.2) (23.2)\n",
            "Collecting pyyaml>=5.1 (from transformers==4.33.2)\n",
            "  Using cached PyYAML-6.0.1-cp312-cp312-win_amd64.whl.metadata (2.1 kB)\n",
            "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\arjun\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers==4.33.2) (2023.10.3)\n",
            "Requirement already satisfied: requests in c:\\users\\arjun\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers==4.33.2) (2.31.0)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers==4.33.2)\n",
            "  Using cached tokenizers-0.13.3.tar.gz (314 kB)\n",
            "  Installing build dependencies: started\n",
            "  Installing build dependencies: finished with status 'done'\n",
            "  Getting requirements to build wheel: started\n",
            "  Getting requirements to build wheel: finished with status 'done'\n",
            "  Preparing metadata (pyproject.toml): started\n",
            "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
            "Collecting safetensors>=0.3.1 (from transformers==4.33.2)\n",
            "  Using cached safetensors-0.4.1.tar.gz (62 kB)\n",
            "  Installing build dependencies: started\n",
            "  Installing build dependencies: finished with status 'done'\n",
            "  Getting requirements to build wheel: started\n",
            "  Getting requirements to build wheel: finished with status 'done'\n",
            "  Preparing metadata (pyproject.toml): started\n",
            "  Preparing metadata (pyproject.toml): finished with status 'error'\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  error: subprocess-exited-with-error\n",
            "  \n",
            "  × Preparing metadata (pyproject.toml) did not run successfully.\n",
            "  │ exit code: 1\n",
            "  ╰─> [6 lines of output]\n",
            "      Checking for Rust toolchain....\n",
            "      \n",
            "      Cargo, the Rust package manager, is not installed or is not on PATH.\n",
            "      This package requires Rust and Cargo to compile extensions. Install it through\n",
            "      the system's package manager or via https://rustup.rs/\n",
            "      \n",
            "      [end of output]\n",
            "  \n",
            "  note: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "error: metadata-generation-failed\n",
            "\n",
            "× Encountered error while generating package metadata.\n",
            "╰─> See above for output.\n",
            "\n",
            "note: This is an issue with the package mentioned above, not pip.\n",
            "hint: See above for details.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting langchain==0.0.299Note: you may need to restart the kernel to use updated packages.\n",
            "\n",
            "  Using cached langchain-0.0.299-py3-none-any.whl.metadata (15 kB)\n",
            "Collecting PyYAML>=5.3 (from langchain==0.0.299)\n",
            "  Using cached PyYAML-6.0.1-cp312-cp312-win_amd64.whl.metadata (2.1 kB)\n",
            "Collecting SQLAlchemy<3,>=1.4 (from langchain==0.0.299)\n",
            "  Using cached SQLAlchemy-2.0.23-cp312-cp312-win_amd64.whl.metadata (9.8 kB)\n",
            "Collecting aiohttp<4.0.0,>=3.8.3 (from langchain==0.0.299)\n",
            "  Using cached aiohttp-3.9.1-cp312-cp312-win_amd64.whl.metadata (7.6 kB)\n",
            "Collecting anyio<4.0 (from langchain==0.0.299)\n",
            "  Using cached anyio-3.7.1-py3-none-any.whl.metadata (4.7 kB)\n",
            "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in c:\\users\\arjun\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from langchain==0.0.299) (0.6.3)\n",
            "Collecting jsonpatch<2.0,>=1.33 (from langchain==0.0.299)\n",
            "  Using cached jsonpatch-1.33-py2.py3-none-any.whl.metadata (3.0 kB)\n",
            "Collecting langsmith<0.1.0,>=0.0.38 (from langchain==0.0.299)\n",
            "  Using cached langsmith-0.0.67-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting numexpr<3.0.0,>=2.8.4 (from langchain==0.0.299)\n",
            "  Using cached numexpr-2.8.7-cp312-cp312-win_amd64.whl.metadata (8.9 kB)\n",
            "Requirement already satisfied: numpy<2,>=1 in c:\\users\\arjun\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from langchain==0.0.299) (1.26.2)\n",
            "Collecting pydantic<3,>=1 (from langchain==0.0.299)\n",
            "  Using cached pydantic-2.5.2-py3-none-any.whl.metadata (65 kB)\n",
            "Requirement already satisfied: requests<3,>=2 in c:\\users\\arjun\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from langchain==0.0.299) (2.31.0)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in c:\\users\\arjun\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from langchain==0.0.299) (8.2.3)\n",
            "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\arjun\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.299) (23.1.0)\n",
            "Collecting multidict<7.0,>=4.5 (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.299)\n",
            "  Using cached multidict-6.0.4.tar.gz (51 kB)\n",
            "  Installing build dependencies: started\n",
            "  Installing build dependencies: finished with status 'done'\n",
            "  Getting requirements to build wheel: started\n",
            "  Getting requirements to build wheel: finished with status 'done'\n",
            "  Installing backend dependencies: started\n",
            "  Installing backend dependencies: finished with status 'done'\n",
            "  Preparing metadata (pyproject.toml): started\n",
            "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
            "Collecting yarl<2.0,>=1.0 (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.299)\n",
            "  Using cached yarl-1.9.3-cp312-cp312-win_amd64.whl.metadata (29 kB)\n",
            "Collecting frozenlist>=1.1.1 (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.299)\n",
            "  Using cached frozenlist-1.4.0.tar.gz (90 kB)\n",
            "  Installing build dependencies: started\n",
            "  Installing build dependencies: finished with status 'done'\n",
            "  Getting requirements to build wheel: started\n",
            "  Getting requirements to build wheel: finished with status 'done'\n",
            "  Preparing metadata (pyproject.toml): started\n",
            "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
            "Collecting aiosignal>=1.1.2 (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.299)\n",
            "  Using cached aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
            "Requirement already satisfied: idna>=2.8 in c:\\users\\arjun\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from anyio<4.0->langchain==0.0.299) (3.6)\n",
            "Collecting sniffio>=1.1 (from anyio<4.0->langchain==0.0.299)\n",
            "  Using cached sniffio-1.3.0-py3-none-any.whl (10 kB)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in c:\\users\\arjun\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from dataclasses-json<0.7,>=0.5.7->langchain==0.0.299) (3.20.1)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in c:\\users\\arjun\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from dataclasses-json<0.7,>=0.5.7->langchain==0.0.299) (0.9.0)\n",
            "Collecting jsonpointer>=1.9 (from jsonpatch<2.0,>=1.33->langchain==0.0.299)\n",
            "  Using cached jsonpointer-2.4-py2.py3-none-any.whl.metadata (2.5 kB)\n",
            "Collecting annotated-types>=0.4.0 (from pydantic<3,>=1->langchain==0.0.299)\n",
            "  Using cached annotated_types-0.6.0-py3-none-any.whl.metadata (12 kB)\n",
            "Collecting pydantic-core==2.14.5 (from pydantic<3,>=1->langchain==0.0.299)\n",
            "  Using cached pydantic_core-2.14.5-cp312-none-win_amd64.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in c:\\users\\arjun\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pydantic<3,>=1->langchain==0.0.299) (4.8.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\arjun\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests<3,>=2->langchain==0.0.299) (3.3.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\arjun\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests<3,>=2->langchain==0.0.299) (2.1.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\arjun\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests<3,>=2->langchain==0.0.299) (2023.11.17)\n",
            "Collecting greenlet!=0.4.17 (from SQLAlchemy<3,>=1.4->langchain==0.0.299)\n",
            "  Using cached greenlet-3.0.1-cp312-cp312-win_amd64.whl.metadata (3.8 kB)\n",
            "Requirement already satisfied: packaging>=17.0 in c:\\users\\arjun\\appdata\\roaming\\python\\python312\\site-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json<0.7,>=0.5.7->langchain==0.0.299) (23.2)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in c:\\users\\arjun\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain==0.0.299) (1.0.0)\n",
            "Using cached langchain-0.0.299-py3-none-any.whl (1.7 MB)\n",
            "Using cached aiohttp-3.9.1-cp312-cp312-win_amd64.whl (362 kB)\n",
            "Using cached anyio-3.7.1-py3-none-any.whl (80 kB)\n",
            "Using cached jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
            "Using cached langsmith-0.0.67-py3-none-any.whl (47 kB)\n",
            "Using cached numexpr-2.8.7-cp312-cp312-win_amd64.whl (95 kB)\n",
            "Using cached pydantic-2.5.2-py3-none-any.whl (381 kB)\n",
            "Using cached pydantic_core-2.14.5-cp312-none-win_amd64.whl (1.9 MB)\n",
            "Using cached PyYAML-6.0.1-cp312-cp312-win_amd64.whl (138 kB)\n",
            "Using cached SQLAlchemy-2.0.23-cp312-cp312-win_amd64.whl (2.1 MB)\n",
            "Using cached annotated_types-0.6.0-py3-none-any.whl (12 kB)\n",
            "Using cached greenlet-3.0.1-cp312-cp312-win_amd64.whl (288 kB)\n",
            "Using cached jsonpointer-2.4-py2.py3-none-any.whl (7.8 kB)\n",
            "Using cached yarl-1.9.3-cp312-cp312-win_amd64.whl (75 kB)\n",
            "Building wheels for collected packages: frozenlist, multidict\n",
            "  Building wheel for frozenlist (pyproject.toml): started\n",
            "  Building wheel for frozenlist (pyproject.toml): finished with status 'error'\n",
            "  Building wheel for multidict (pyproject.toml): started\n",
            "  Building wheel for multidict (pyproject.toml): finished with status 'error'\n",
            "Failed to build frozenlist multidict\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  error: subprocess-exited-with-error\n",
            "  \n",
            "  × Building wheel for frozenlist (pyproject.toml) did not run successfully.\n",
            "  │ exit code: 1\n",
            "  ╰─> [33 lines of output]\n",
            "      *********************\n",
            "      * Accelerated build *\n",
            "      *********************\n",
            "      running bdist_wheel\n",
            "      running build\n",
            "      running build_py\n",
            "      creating build\n",
            "      creating build\\lib.win-amd64-cpython-312\n",
            "      creating build\\lib.win-amd64-cpython-312\\frozenlist\n",
            "      copying frozenlist\\__init__.py -> build\\lib.win-amd64-cpython-312\\frozenlist\n",
            "      running egg_info\n",
            "      writing frozenlist.egg-info\\PKG-INFO\n",
            "      writing dependency_links to frozenlist.egg-info\\dependency_links.txt\n",
            "      writing top-level names to frozenlist.egg-info\\top_level.txt\n",
            "      reading manifest file 'frozenlist.egg-info\\SOURCES.txt'\n",
            "      reading manifest template 'MANIFEST.in'\n",
            "      warning: no previously-included files matching '*.pyc' found anywhere in distribution\n",
            "      warning: no previously-included files matching '*.pyd' found anywhere in distribution\n",
            "      warning: no previously-included files matching '*.so' found anywhere in distribution\n",
            "      warning: no previously-included files matching '*.lib' found anywhere in distribution\n",
            "      warning: no previously-included files matching '*.dll' found anywhere in distribution\n",
            "      warning: no previously-included files matching '*.a' found anywhere in distribution\n",
            "      warning: no previously-included files matching '*.obj' found anywhere in distribution\n",
            "      warning: no previously-included files found matching 'frozenlist\\*.html'\n",
            "      no previously-included directories found matching 'docs\\_build'\n",
            "      adding license file 'LICENSE'\n",
            "      writing manifest file 'frozenlist.egg-info\\SOURCES.txt'\n",
            "      copying frozenlist\\__init__.pyi -> build\\lib.win-amd64-cpython-312\\frozenlist\n",
            "      copying frozenlist\\_frozenlist.pyx -> build\\lib.win-amd64-cpython-312\\frozenlist\n",
            "      copying frozenlist\\py.typed -> build\\lib.win-amd64-cpython-312\\frozenlist\n",
            "      running build_ext\n",
            "      building 'frozenlist._frozenlist' extension\n",
            "      error: Microsoft Visual C++ 14.0 or greater is required. Get it with \"Microsoft C++ Build Tools\": https://visualstudio.microsoft.com/visual-cpp-build-tools/\n",
            "      [end of output]\n",
            "  \n",
            "  note: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "  ERROR: Failed building wheel for frozenlist\n",
            "  error: subprocess-exited-with-error\n",
            "  \n",
            "  × Building wheel for multidict (pyproject.toml) did not run successfully.\n",
            "  │ exit code: 1\n",
            "  ╰─> [74 lines of output]\n",
            "      *********************\n",
            "      * Accelerated build *\n",
            "      *********************\n",
            "      running bdist_wheel\n",
            "      running build\n",
            "      running build_py\n",
            "      creating build\n",
            "      creating build\\lib.win-amd64-cpython-312\n",
            "      creating build\\lib.win-amd64-cpython-312\\multidict\n",
            "      copying multidict\\_abc.py -> build\\lib.win-amd64-cpython-312\\multidict\n",
            "      copying multidict\\_compat.py -> build\\lib.win-amd64-cpython-312\\multidict\n",
            "      copying multidict\\_multidict_base.py -> build\\lib.win-amd64-cpython-312\\multidict\n",
            "      copying multidict\\_multidict_py.py -> build\\lib.win-amd64-cpython-312\\multidict\n",
            "      copying multidict\\__init__.py -> build\\lib.win-amd64-cpython-312\\multidict\n",
            "      running egg_info\n",
            "      writing multidict.egg-info\\PKG-INFO\n",
            "      writing dependency_links to multidict.egg-info\\dependency_links.txt\n",
            "      writing top-level names to multidict.egg-info\\top_level.txt\n",
            "      reading manifest file 'multidict.egg-info\\SOURCES.txt'\n",
            "      reading manifest template 'MANIFEST.in'\n",
            "      warning: no previously-included files matching '*.pyc' found anywhere in distribution\n",
            "      warning: no previously-included files found matching 'multidict\\_multidict.html'\n",
            "      warning: no previously-included files found matching 'multidict\\*.so'\n",
            "      warning: no previously-included files found matching 'multidict\\*.pyd'\n",
            "      warning: no previously-included files found matching 'multidict\\*.pyd'\n",
            "      no previously-included directories found matching 'docs\\_build'\n",
            "      adding license file 'LICENSE'\n",
            "      writing manifest file 'multidict.egg-info\\SOURCES.txt'\n",
            "      C:\\Users\\Arjun\\AppData\\Local\\Temp\\pip-build-env-2wlcf2tb\\overlay\\Lib\\site-packages\\setuptools\\command\\build_py.py:207: _Warning: Package 'multidict._multilib' is absent from the `packages` configuration.\n",
            "      !!\n",
            "      \n",
            "              ********************************************************************************\n",
            "              ############################\n",
            "              # Package would be ignored #\n",
            "              ############################\n",
            "              Python recognizes 'multidict._multilib' as an importable package[^1],\n",
            "              but it is absent from setuptools' `packages` configuration.\n",
            "      \n",
            "              This leads to an ambiguous overall configuration. If you want to distribute this\n",
            "              package, please make sure that 'multidict._multilib' is explicitly added\n",
            "              to the `packages` configuration field.\n",
            "      \n",
            "              Alternatively, you can also rely on setuptools' discovery methods\n",
            "              (for example by using `find_namespace_packages(...)`/`find_namespace:`\n",
            "              instead of `find_packages(...)`/`find:`).\n",
            "      \n",
            "              You can read more about \"package discovery\" on setuptools documentation page:\n",
            "      \n",
            "              - https://setuptools.pypa.io/en/latest/userguide/package_discovery.html\n",
            "      \n",
            "              If you don't want 'multidict._multilib' to be distributed and are\n",
            "              already explicitly excluding 'multidict._multilib' via\n",
            "              `find_namespace_packages(...)/find_namespace` or `find_packages(...)/find`,\n",
            "              you can try to use `exclude_package_data`, or `include-package-data=False` in\n",
            "              combination with a more fine grained `package-data` configuration.\n",
            "      \n",
            "              You can read more about \"package data files\" on setuptools documentation page:\n",
            "      \n",
            "              - https://setuptools.pypa.io/en/latest/userguide/datafiles.html\n",
            "      \n",
            "      \n",
            "              [^1]: For Python, any directory (with suitable naming) can be imported,\n",
            "                    even if it does not contain any `.py` files.\n",
            "                    On the other hand, currently there is no concept of package data\n",
            "                    directory, all directories are treated like packages.\n",
            "              ********************************************************************************\n",
            "      \n",
            "      !!\n",
            "        check.warn(importable)\n",
            "      copying multidict\\__init__.pyi -> build\\lib.win-amd64-cpython-312\\multidict\n",
            "      copying multidict\\py.typed -> build\\lib.win-amd64-cpython-312\\multidict\n",
            "      running build_ext\n",
            "      building 'multidict._multidict' extension\n",
            "      error: Microsoft Visual C++ 14.0 or greater is required. Get it with \"Microsoft C++ Build Tools\": https://visualstudio.microsoft.com/visual-cpp-build-tools/\n",
            "      [end of output]\n",
            "  \n",
            "  note: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "  ERROR: Failed building wheel for multidict\n",
            "ERROR: Could not build wheels for frozenlist, multidict, which is required to install pyproject.toml-based projects\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting chromadb==0.4.10\n",
            "  Using cached chromadb-0.4.10-py3-none-any.whl.metadata (7.0 kB)\n",
            "Requirement already satisfied: requests>=2.28 in c:\\users\\arjun\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from chromadb==0.4.10) (2.31.0)\n",
            "Collecting pydantic<2.0,>=1.9 (from chromadb==0.4.10)\n",
            "  Using cached pydantic-1.10.13-py3-none-any.whl.metadata (149 kB)\n",
            "Collecting chroma-hnswlib==0.7.3 (from chromadb==0.4.10)\n",
            "  Using cached chroma-hnswlib-0.7.3.tar.gz (31 kB)\n",
            "  Installing build dependencies: started\n",
            "  Installing build dependencies: finished with status 'done'\n",
            "  Getting requirements to build wheel: started\n",
            "  Getting requirements to build wheel: finished with status 'done'\n",
            "  Preparing metadata (pyproject.toml): started\n",
            "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
            "Collecting fastapi<0.100.0,>=0.95.2 (from chromadb==0.4.10)\n",
            "  Using cached fastapi-0.99.1-py3-none-any.whl.metadata (23 kB)\n",
            "Collecting uvicorn>=0.18.3 (from uvicorn[standard]>=0.18.3->chromadb==0.4.10)\n",
            "  Using cached uvicorn-0.24.0.post1-py3-none-any.whl.metadata (6.4 kB)\n",
            "Collecting posthog>=2.4.0 (from chromadb==0.4.10)\n",
            "  Using cached posthog-3.0.2-py2.py3-none-any.whl.metadata (2.0 kB)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in c:\\users\\arjun\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from chromadb==0.4.10) (4.8.0)\n",
            "INFO: pip is looking at multiple versions of chromadb to determine which version is compatible with other requirements. This could take a while.\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "ERROR: Could not find a version that satisfies the requirement pulsar-client>=3.1.0 (from chromadb) (from versions: none)\n",
            "ERROR: No matching distribution found for pulsar-client>=3.1.0\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting xformers==0.0.21\n",
            "  Using cached xformers-0.0.21.tar.gz (22.3 MB)\n",
            "  Installing build dependencies: started\n",
            "  Installing build dependencies: finished with status 'done'\n",
            "  Getting requirements to build wheel: started\n",
            "  Getting requirements to build wheel: finished with status 'error'\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  error: subprocess-exited-with-error\n",
            "  \n",
            "  × Getting requirements to build wheel did not run successfully.\n",
            "  │ exit code: 1\n",
            "  ╰─> [20 lines of output]\n",
            "      Traceback (most recent call last):\n",
            "        File \"c:\\Users\\Arjun\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pip\\_vendor\\pyproject_hooks\\_in_process\\_in_process.py\", line 353, in <module>\n",
            "          main()\n",
            "        File \"c:\\Users\\Arjun\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pip\\_vendor\\pyproject_hooks\\_in_process\\_in_process.py\", line 335, in main\n",
            "          json_out['return_val'] = hook(**hook_input['kwargs'])\n",
            "                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "        File \"c:\\Users\\Arjun\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pip\\_vendor\\pyproject_hooks\\_in_process\\_in_process.py\", line 118, in get_requires_for_build_wheel\n",
            "          return hook(config_settings)\n",
            "                 ^^^^^^^^^^^^^^^^^^^^^\n",
            "        File \"C:\\Users\\Arjun\\AppData\\Local\\Temp\\pip-build-env-31kxgh65\\overlay\\Lib\\site-packages\\setuptools\\build_meta.py\", line 325, in get_requires_for_build_wheel\n",
            "          return self._get_build_requires(config_settings, requirements=['wheel'])\n",
            "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "        File \"C:\\Users\\Arjun\\AppData\\Local\\Temp\\pip-build-env-31kxgh65\\overlay\\Lib\\site-packages\\setuptools\\build_meta.py\", line 295, in _get_build_requires\n",
            "          self.run_setup()\n",
            "        File \"C:\\Users\\Arjun\\AppData\\Local\\Temp\\pip-build-env-31kxgh65\\overlay\\Lib\\site-packages\\setuptools\\build_meta.py\", line 480, in run_setup\n",
            "          super(_BuildMetaLegacyBackend, self).run_setup(setup_script=setup_script)\n",
            "        File \"C:\\Users\\Arjun\\AppData\\Local\\Temp\\pip-build-env-31kxgh65\\overlay\\Lib\\site-packages\\setuptools\\build_meta.py\", line 311, in run_setup\n",
            "          exec(code, locals())\n",
            "        File \"<string>\", line 23, in <module>\n",
            "      ModuleNotFoundError: No module named 'torch'\n",
            "      [end of output]\n",
            "  \n",
            "  note: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "error: subprocess-exited-with-error\n",
            "\n",
            "× Getting requirements to build wheel did not run successfully.\n",
            "│ exit code: 1\n",
            "╰─> See above for output.\n",
            "\n",
            "note: This error originates from a subprocess, and is likely not a problem with pip.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting sentence_transformers==2.2.2\n",
            "  Using cached sentence-transformers-2.2.2.tar.gz (85 kB)\n",
            "  Installing build dependencies: started\n",
            "  Installing build dependencies: finished with status 'done'\n",
            "  Getting requirements to build wheel: started\n",
            "  Getting requirements to build wheel: finished with status 'done'\n",
            "  Preparing metadata (pyproject.toml): started\n",
            "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
            "Collecting transformers<5.0.0,>=4.6.0 (from sentence_transformers==2.2.2)\n",
            "  Using cached transformers-4.35.2-py3-none-any.whl.metadata (123 kB)\n",
            "Requirement already satisfied: tqdm in c:\\users\\arjun\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from sentence_transformers==2.2.2) (4.66.1)\n",
            "INFO: pip is looking at multiple versions of sentence-transformers to determine which version is compatible with other requirements. This could take a while.\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "ERROR: Could not find a version that satisfies the requirement torch>=1.6.0 (from sentence-transformers) (from versions: none)\n",
            "ERROR: No matching distribution found for torch>=1.6.0\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting tokenizers==0.14.0\n",
            "  Using cached tokenizers-0.14.0.tar.gz (286 kB)\n",
            "  Installing build dependencies: started\n",
            "  Installing build dependencies: finished with status 'done'\n",
            "  Getting requirements to build wheel: started\n",
            "  Getting requirements to build wheel: finished with status 'done'\n",
            "  Preparing metadata (pyproject.toml): started\n",
            "  Preparing metadata (pyproject.toml): finished with status 'error'\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  error: subprocess-exited-with-error\n",
            "  \n",
            "  × Preparing metadata (pyproject.toml) did not run successfully.\n",
            "  │ exit code: 1\n",
            "  ╰─> [6 lines of output]\n",
            "      Checking for Rust toolchain....\n",
            "      \n",
            "      Cargo, the Rust package manager, is not installed or is not on PATH.\n",
            "      This package requires Rust and Cargo to compile extensions. Install it through\n",
            "      the system's package manager or via https://rustup.rs/\n",
            "      \n",
            "      [end of output]\n",
            "  \n",
            "  note: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "error: metadata-generation-failed\n",
            "\n",
            "× Encountered error while generating package metadata.\n",
            "╰─> See above for output.\n",
            "\n",
            "note: This is an issue with the package mentioned above, not pip.\n",
            "hint: See above for details.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting optimum==1.13.1Note: you may need to restart the kernel to use updated packages.\n",
            "\n",
            "  Using cached optimum-1.13.1-py3-none-any.whl.metadata (17 kB)\n",
            "Collecting coloredlogs (from optimum==1.13.1)\n",
            "  Using cached coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
            "Collecting sympy (from optimum==1.13.1)\n",
            "  Using cached sympy-1.12-py3-none-any.whl (5.7 MB)\n",
            "Collecting transformers>=4.26.0 (from transformers[sentencepiece]>=4.26.0->optimum==1.13.1)\n",
            "  Using cached transformers-4.35.2-py3-none-any.whl.metadata (123 kB)\n",
            "INFO: pip is looking at multiple versions of optimum to determine which version is compatible with other requirements. This could take a while.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "ERROR: Could not find a version that satisfies the requirement torch>=1.9 (from optimum) (from versions: none)\n",
            "ERROR: No matching distribution found for torch>=1.9\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://huggingface.github.io/autogptq-index/whl/cu118/Note: you may need to restart the kernel to use updated packages.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "ERROR: Could not find a version that satisfies the requirement auto-gptq==0.4.2 (from versions: 0.0.4, 0.0.5, 0.1.0, 0.2.0, 0.2.1, 0.2.2, 0.3.0, 0.3.1, 0.3.2, 0.5.0, 0.5.1)\n",
            "ERROR: No matching distribution found for auto-gptq==0.4.2\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Requirement already satisfied: unstructured==0.10.16 in c:\\users\\arjun\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (0.10.16)\n",
            "Requirement already satisfied: chardet in c:\\users\\arjun\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from unstructured==0.10.16) (5.2.0)\n",
            "Requirement already satisfied: filetype in c:\\users\\arjun\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from unstructured==0.10.16) (1.2.0)\n",
            "Requirement already satisfied: python-magic in c:\\users\\arjun\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from unstructured==0.10.16) (0.4.27)\n",
            "Requirement already satisfied: lxml in c:\\users\\arjun\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from unstructured==0.10.16) (4.9.3)\n",
            "Requirement already satisfied: nltk in c:\\users\\arjun\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from unstructured==0.10.16) (3.8.1)\n",
            "Requirement already satisfied: tabulate in c:\\users\\arjun\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from unstructured==0.10.16) (0.9.0)\n",
            "Requirement already satisfied: requests in c:\\users\\arjun\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from unstructured==0.10.16) (2.31.0)\n",
            "Requirement already satisfied: beautifulsoup4 in c:\\users\\arjun\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from unstructured==0.10.16) (4.12.2)\n",
            "Requirement already satisfied: emoji in c:\\users\\arjun\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from unstructured==0.10.16) (2.8.0)\n",
            "Requirement already satisfied: dataclasses-json in c:\\users\\arjun\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from unstructured==0.10.16) (0.6.3)\n",
            "Requirement already satisfied: python-iso639 in c:\\users\\arjun\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from unstructured==0.10.16) (2023.6.15)\n",
            "Requirement already satisfied: soupsieve>1.2 in c:\\users\\arjun\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from beautifulsoup4->unstructured==0.10.16) (2.5)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in c:\\users\\arjun\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from dataclasses-json->unstructured==0.10.16) (3.20.1)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in c:\\users\\arjun\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from dataclasses-json->unstructured==0.10.16) (0.9.0)\n",
            "Requirement already satisfied: click in c:\\users\\arjun\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from nltk->unstructured==0.10.16) (8.1.7)\n",
            "Requirement already satisfied: joblib in c:\\users\\arjun\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from nltk->unstructured==0.10.16) (1.3.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\arjun\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from nltk->unstructured==0.10.16) (2023.10.3)\n",
            "Requirement already satisfied: tqdm in c:\\users\\arjun\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from nltk->unstructured==0.10.16) (4.66.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\arjun\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->unstructured==0.10.16) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\arjun\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->unstructured==0.10.16) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\arjun\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->unstructured==0.10.16) (2.1.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\arjun\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->unstructured==0.10.16) (2023.11.17)\n",
            "Requirement already satisfied: packaging>=17.0 in c:\\users\\arjun\\appdata\\roaming\\python\\python312\\site-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json->unstructured==0.10.16) (23.2)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in c:\\users\\arjun\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json->unstructured==0.10.16) (1.0.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4 in c:\\users\\arjun\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json->unstructured==0.10.16) (4.8.0)\n",
            "Requirement already satisfied: colorama in c:\\users\\arjun\\appdata\\roaming\\python\\python312\\site-packages (from click->nltk->unstructured==0.10.16) (0.4.6)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "%pip install torch==2.0.1\n",
        "%pip install transformers==4.33.2\n",
        "%pip install langchain==0.0.299\n",
        "%pip install chromadb==0.4.10\n",
        "%pip install xformers==0.0.21\n",
        "%pip install sentence_transformers==2.2.2\n",
        "%pip install tokenizers==0.14.0\n",
        "%pip install optimum==1.13.1\n",
        "%pip install auto-gptq==0.4.2 --extra-index-url https://huggingface.github.io/autogptq-index/whl/cu118/\n",
        "%pip install unstructured==0.10.16"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PZ-IYQwp65Fk",
        "outputId": "e64910e8-af68-49b9-9463-09b371aa1006"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "ERROR: Could not install packages due to an OSError: [Errno 28] No space left on device\n",
            "\n"
          ]
        }
      ],
      "source": [
        "%pip install -q streamlit"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DAtoGD3EumIw",
        "outputId": "59e89081-bdf8-4352-ced6-5dc155f890db"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Writing app.py\n"
          ]
        }
      ],
      "source": [
        "import streamlit as st\n",
        "import torch\n",
        "from langchain import HuggingFacePipeline\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, GenerationConfig, pipeline\n",
        "from langchain.document_loaders import UnstructuredMarkdownLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.vectorstores import Chroma\n",
        "from langchain.chains import RetrievalQA, LLMChain\n",
        "from langchain import PromptTemplate\n",
        "\n",
        "MODEL_NAME = \"TheBloke/Llama-2-7b-Chat-GPTQ\"\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "@st.cache(allow_output_mutation=True)\n",
        "def load_model_and_tokenizer():\n",
        "    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True)\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        MODEL_NAME, torch_dtype=torch.float16, trust_remote_code=True, device_map=\"auto\"\n",
        "    )\n",
        "    generation_config = GenerationConfig.from_pretrained(MODEL_NAME)\n",
        "    generation_config.max_new_tokens = 256\n",
        "    generation_config.temperature = 0.5\n",
        "    generation_config.top_p = 0.95\n",
        "    generation_config.do_sample = True\n",
        "    generation_config.repetition_penalty = 1.15\n",
        "    text_pipeline = pipeline(\n",
        "        \"text-generation\",\n",
        "        model=model,\n",
        "        tokenizer=tokenizer,\n",
        "        generation_config=generation_config,\n",
        "    )\n",
        "    return HuggingFacePipeline(pipeline=text_pipeline, model_kwargs={\"temperature\": 0})\n",
        "\n",
        "llm = load_model_and_tokenizer()\n",
        "\n",
        "# Load and process your data outside the main function\n",
        "loader = UnstructuredMarkdownLoader(\"/content/questions.csv\")\n",
        "docs = loader.load()\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1024, chunk_overlap=64)\n",
        "texts = text_splitter.split_documents(docs)\n",
        "\n",
        "embeddings = HuggingFaceEmbeddings(\n",
        "    model_name=\"thenlper/gte-large\",\n",
        "    model_kwargs={\"device\": DEVICE},\n",
        "    encode_kwargs={\"normalize_embeddings\": True},\n",
        ")\n",
        "\n",
        "# Consider if this part can be preprocessed or cached\n",
        "db = Chroma.from_documents(texts, embeddings, persist_directory=\"db\")\n",
        "\n",
        "prompt_template = \"\"\"\n",
        "<s>[INST] <<SYS>>\n",
        "You are expert in Human development index. You have basic to advance knowledge to answer related to human development index (HDI).\n",
        "<</SYS>>\n",
        "\n",
        "{context}\n",
        "\n",
        "{question} [/INST]\n",
        "\"\"\"\n",
        "\n",
        "prompt = PromptTemplate(template=prompt_template, input_variables=[\"context\", \"question\"])\n",
        "\n",
        "qa_chain = RetrievalQA.from_chain_type(\n",
        "    llm=llm,\n",
        "    chain_type=\"stuff\",\n",
        "    retriever=db.as_retriever(search_kwargs={\"k\": 5}),\n",
        "    return_source_documents=True,\n",
        "    chain_type_kwargs={\"prompt\": prompt},\n",
        ")\n",
        "\n",
        "def main():\n",
        "    st.title(\"HDI\")\n",
        "    q = st.text_input(\"Question\")\n",
        "    if st.button(\"Submit\"):\n",
        "        result = qa_chain(q)\n",
        "        st.write(result[\"result\"].strip())\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nyDBGPDXurHz",
        "outputId": "fdba113d-b2e4-48d5-b741-bbd5bb3b4ecf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "34.125.191.50\n",
            "\u001b[K\u001b[?25hnpx: installed 22 in 3.12s\n",
            "your url is: https://nasty-turkeys-pay.loca.lt\n",
            "/root/.npm/_npx/2230/lib/node_modules/localtunnel/bin/lt.js:81\n",
            "    throw err;\n",
            "    ^\n",
            "\n",
            "Error: connection refused: localtunnel.me:34523 (check your firewall settings)\n",
            "    at Socket.<anonymous> (/root/.npm/_npx/2230/lib/node_modules/\u001b[4mlocaltunnel\u001b[24m/lib/TunnelCluster.js:52:11)\n",
            "\u001b[90m    at Socket.emit (events.js:315:20)\u001b[39m\n",
            "\u001b[90m    at emitErrorNT (internal/streams/destroy.js:106:8)\u001b[39m\n",
            "\u001b[90m    at emitErrorCloseNT (internal/streams/destroy.js:74:3)\u001b[39m\n",
            "\u001b[90m    at processTicksAndRejections (internal/process/task_queues.js:80:21)\u001b[39m\n"
          ]
        }
      ],
      "source": [
        "!streamlit run app.py &>/content/logs.txt & npx localtunnel --port 8501 & curl ipv4.icanhazip.com"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sa8j68xEgnVy",
        "outputId": "f1150523-89b5-4e3f-c3fa-9e0b2aaa8a61"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/bin/bash: line 1: streamlit: command not found\n"
          ]
        }
      ],
      "source": [
        "!streamlit run app.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-KiG0o7AjRl0"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
